\section{Related Work}

This section will try to show more clearly the place the work takes among adjacent works.

Firstly, the work utilizes distillation technique to provide automatic fusion of sparse linear algebra routines. At the moment, we are not aware of other works that focus on sparse linear algebra routines fusion from implementation perspective. However, it is worth noticing that SuiteSparse addresses this problem in future work~\cite{newsuitesparse}. The approaches that work well for dense routines fusion struggle with index arithmetic, e.g.~\cite{Futhark}, induced by sparse representation. Other approaches makes specific assumption about the operands or require to express computations with the help of combinators~\cite{StreamFus}. Our approach makes no assumptions and was shown to work successfully.

Secondly, in contrast with other works, e.g.\cite{zhang2020sparch}, this work opts for functional high-level synthesis to produce hardware. Thus, we are not limited to only specific kernels like matrix-vector multiplication. FHW is not the only functional high-level synthesis compiler, however, it appeared to be the best fit for our pipeline. A more detailed discussion of alternatives could be found at~\cite{Edwards2019FHWP}. Although we have slightly worse performance than software counterparts, the approach has a certain potential. For the same reason, we do not provide any comparison with other hardware works at the moment. Also, they are too specialized, e.g. they provide only matrix multiplication operations, while we focus on providing a reasonable GraphBLAS subset.

Finally,~\cite{superreduceron} evaluates whether supercompilation makes any benefits when a functional program is executed on a dedicated reduction-based processor \textit{Reduceron}. Unlike this work, we use another hardware backend, distillation, and choose specific programs, namely those that contain sparse linear algebra routines. However, we plan to perform the same evaluation with Reduceron as a backend.