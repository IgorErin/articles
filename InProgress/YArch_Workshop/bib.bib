@book{GAILLA,
author = {Kepner, Jeremy and Gilbert, John},
title = {Graph Algorithms in the Language of Linear Algebra},
year = {2011},
isbn = {0898719909},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {Graphs are among the most important abstract data types in computer science, and the algorithms that operate on them are critical to modern life. Graphs have been shown to be powerful tools for modeling complex problems because of their simplicity and generality. Graph algorithms are one of the pillars of mathematics, informing research in such diverse areas as combinatorial optimization, complexity theory, and topology. Algorithms on graphs are applied in many ways in today s world - from Web rankings to metabolic networks, from finite element meshes to semantic graphs. The current exponential growth in graph data has forced a shift to parallel computing for executing graph algorithms. Implementing parallel graph algorithms and achieving good parallel performance have proven difficult. This book addresses these challenges by exploiting the well-known duality between a canonical representation of graphs as abstract collections of vertices and edges and a sparse adjacency matrix representation. This linear algebraic approach is widely accessible to scientists and engineers who may not be formally trained in computer science. The authors show how to leverage existing parallel matrix computation techniques and the large amount of software infrastructure that exists for these computations to implement efficient and scalable parallel graph algorithms. The benefits of this approach are reduced algorithmic complexity, ease of implementation, and improved performance. Graph Algorithms in the Language of Linear Algebra is the first book to cover graph algorithms accessible to engineers and scientists not trained in computer science but having a strong linear algebra background, enabling them to quickly understand and apply graph algorithms. It also covers array-based graph algorithms, showing readers how to express canonical graph algorithms using a highly elegant and efficient array notation and how to tap into the large range of tools and techniques that have been built for matrices and tensors; parallel array-based algorithms, demonstrating with examples how to easily implement parallel graph algorithms using array-based approaches, which enables readers to address much larger graph problems; and array-based theory for analyzing graphs, providing a template for using array-based constructs to develop new theoretical approaches for graph analysis. Audience: This book is suitable as the primary text for a class on linear algebraic graph algorithms and as either the primary or supplemental text for a class on graph algorithms for engineers and scientists without training in computer science. Contents: List of Figures; List of Tables; List of Algorithms; Preface; Acknowledgments; Part I: Algorithms: Chapter 1: Graphs and Matrices; Chapter 2: Linear Algebraic Notation and Definitions; Chapter 3: Connected Components and Minimum Paths; Chapter 4: Some Graph Algorithms in an Array-Based Language; Chapter 5: Fundamental Graph Algorithms; Chapter 6: Complex Graph Algorithms; Chapter 7: Multilinear Algebra for Analyzing Data with Multiple Linkages; Chapter 8: Subgraph Detection; Part II: Data: Chapter 9: Kronecker Graphs; Chapter 10: The Kronecker Theory of Power Law Graphs; Chapter 11: Visualizing Large Kronecker Graphs; Part III: Computation: Chapter 12: Large-Scale Network Analysis; Chapter 13: Implementing Sparse Matrices for Graph Algorithms; Chapter 14: New Ideas in Sparse Matrix-Matrix Multiplication; Chapter 15: Parallel Mapping of Sparse Computations; Chapter 16: Fundamental Questions in the Analysis of Large Graphs; Index.}
}

@article{buluc2017graphblas,
  title={The GraphBLAS C API Specification},
  author={Buluc, Aydin and Mattson, Timothy and McMillan, Scott and Moreira, Jose and Yang, Carl},
  journal={GraphBLAS. org, Tech. Rep.},
  year={2017}
}

@misc{yang2020graphblast,
      title={GraphBLAST: A High-Performance Linear Algebra-based Graph Framework on the GPU}, 
      author={Carl Yang and Aydin Buluc and John D. Owens},
      year={2020},
      eprint={1908.01407},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}
@article{KernelFusion,
   title={Optimizing CUDA code by kernel fusion: application on BLAS},
   volume={71},
   ISSN={1573-0484},
   url={http://dx.doi.org/10.1007/s11227-015-1483-z},
   DOI={10.1007/s11227-015-1483-z},
   number={10},
   journal={The Journal of Supercomputing},
   publisher={Springer Science and Business Media LLC},
   author={Filipovič, Jiří and Madzin, Matúš and Fousek, Jan and Matyska, Luděk},
   year={2015},
   month={Jul},
   pages={3934–3957}
}

@inproceedings{10.1145/3062341.3062354,
author = {Henriksen, Troels and Serup, Niels G. W. and Elsman, Martin and Henglein, Fritz and Oancea, Cosmin E.},
title = {Futhark: Purely Functional GPU-Programming with Nested Parallelism and in-Place Array Updates},
year = {2017},
isbn = {9781450349888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3062341.3062354},
doi = {10.1145/3062341.3062354},
abstract = { Futhark is a purely functional data-parallel array language that offers a machine-neutral programming model and an optimising compiler that generates OpenCL code for GPUs.  This paper presents the design and implementation of three key features of Futhark that seek a suitable middle ground with imperative approaches.  First, in order to express efficient code inside the parallel constructs, we introduce a simple type system for in-place updates that ensures referential transparency and supports equational reasoning.  Second, we furnish Futhark with parallel operators capable of expressing efficient strength-reduced code, along with their fusion rules.  Third, we present a flattening transformation aimed at enhancing the degree of parallelism that (i) builds on loop interchange and distribution but uses higher-order reasoning rather than array-dependence analysis, and (ii) still allows further locality-of-reference optimisations. Finally, an evaluation on 16 benchmarks demonstrates the impact of the language and compiler features and shows application-level performance competitive with hand-written GPU code. },
booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {556–571},
numpages = {16},
keywords = {compilers, functional language, parallel, GPGPU},
location = {Barcelona, Spain},
series = {PLDI 2017}
}

@article{Futhark,
author = {Henriksen, Troels and Serup, Niels G. W. and Elsman, Martin and Henglein, Fritz and Oancea, Cosmin E.},
title = {Futhark: Purely Functional GPU-Programming with Nested Parallelism and in-Place Array Updates},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/3140587.3062354},
doi = {10.1145/3140587.3062354},
abstract = { Futhark is a purely functional data-parallel array language that offers a machine-neutral programming model and an optimising compiler that generates OpenCL code for GPUs.  This paper presents the design and implementation of three key features of Futhark that seek a suitable middle ground with imperative approaches.  First, in order to express efficient code inside the parallel constructs, we introduce a simple type system for in-place updates that ensures referential transparency and supports equational reasoning.  Second, we furnish Futhark with parallel operators capable of expressing efficient strength-reduced code, along with their fusion rules.  Third, we present a flattening transformation aimed at enhancing the degree of parallelism that (i) builds on loop interchange and distribution but uses higher-order reasoning rather than array-dependence analysis, and (ii) still allows further locality-of-reference optimisations. Finally, an evaluation on 16 benchmarks demonstrates the impact of the language and compiler features and shows application-level performance competitive with hand-written GPU code. },
journal = {SIGPLAN Not.},
month = jun,
pages = {556–571},
numpages = {16},
keywords = {parallel, functional language, compilers, GPGPU}
}



@inproceedings{CUDADMA,
author = {Bauer, Michael and Cook, Henry and Khailany, Brucek},
title = {CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization},
year = {2011},
isbn = {9781450307710},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063384.2063400},
doi = {10.1145/2063384.2063400},
abstract = {As the computational power of GPUs continues to scale with Moore's Law, an increasing number of applications are becoming limited by memory bandwidth. We propose an approach for programming GPUs with tightly-coupled specialized DMA warps for performing memory transfers between on-chip and off-chip memories. Separate DMA warps improve memory bandwidth utilization by better exploiting available memory-level parallelism and by leveraging efficient inter-warp producer-consumer synchronization mechanisms. DMA warps also improve programmer productivity by decoupling the need for thread array shapes to match data layout. To illustrate the benefits of this approach, we present an extensible API, CudaDMA, that encapsulates synchronization and common sequential and strided data transfer patterns. Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications written in CUDA running on NVIDIA Fermi GPUs.},
booktitle = {Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {12},
numpages = {11},
location = {Seattle, Washington},
series = {SC '11}
}

@article{Song_2016,
   title={Novel graph processor architecture, prototype system, and results},
   ISBN={9781509035250},
   url={http://dx.doi.org/10.1109/HPEC.2016.7761635},
   DOI={10.1109/hpec.2016.7761635},
   journal={2016 IEEE High Performance Extreme Computing Conference (HPEC)},
   publisher={IEEE},
   author={Song, William S. and Gleyzer, Vitaliy and Lomakin, Alexei and Kepner, Jeremy},
   year={2016},
   month={Sep}
}

@misc{leskovec2016snap,
      title={SNAP: A General Purpose Network Analysis and Graph Mining Library}, 
      author={Jure Leskovec and Rok Sosic},
      year={2016},
      eprint={1606.07550},
      archivePrefix={arXiv},
      primaryClass={cs.SI}
}

@article{Florida,
author = {Davis, Timothy A. and Hu, Yifan},
title = {The University of Florida Sparse Matrix Collection},
year = {2011},
issue_date = {November 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/2049662.2049663},
doi = {10.1145/2049662.2049663},
abstract = {We describe the University of Florida Sparse Matrix Collection, a large and actively growing set of sparse matrices that arise in real applications. The Collection is widely used by the numerical linear algebra community for the development and performance evaluation of sparse matrix algorithms. It allows for robust and repeatable experiments: robust because performance results with artificially generated matrices can be misleading, and repeatable because matrices are curated and made publicly available in many formats. Its matrices cover a wide spectrum of domains, include those arising from problems with underlying 2D or 3D geometry (as structural engineering, computational fluid dynamics, model reduction, electromagnetics, semiconductor devices, thermodynamics, materials, acoustics, computer graphics/vision, robotics/kinematics, and other discretizations) and those that typically do not have such geometry (optimization, circuit simulation, economic and financial modeling, theoretical and quantum chemistry, chemical process simulation, mathematics and statistics, power networks, and other networks and graphs). We provide software for accessing and managing the Collection, from MATLAB™, Mathematica™, Fortran, and C, as well as an online search capability. Graph visualization of the matrices is provided, and a new multilevel coarsening scheme is proposed to facilitate this task.},
journal = {ACM Trans. Math. Softw.},
month = dec,
articleno = {1},
numpages = {25},
keywords = {multilevel algorithms, Graph drawing, sparse matrices, performance evaluation}
}

 @inproceedings{zhang2020sparch,
    title     = {SpArch: Efficient Architecture for Sparse Matrix Multiplication},
    author    = {Zhang, Zhekai and Wang, Hanrui and Han, Song and Dally, William J.},
    booktitle = {26th IEEE International Symposium on High Performance Computer Architecture (HPCA)},
    year      = {2020}
} 

@inproceedings{Systolic,
author = {He, Xin and Pal, Subhankar and Amarnath, Aporva and Feng, Siying and Park, Dong-Hyeon and Rovinski, Austin and Ye, Haojie and Chen, Yuhan and Dreslinski, Ronald and Mudge, Trevor},
title = {Sparse-TPU: Adapting Systolic Arrays for Sparse Matrices},
year = {2020},
isbn = {9781450379830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3392717.3392751},
doi = {10.1145/3392717.3392751},
abstract = {While systolic arrays are widely used for dense-matrix operations, they are seldom used for sparse-matrix operations. In this paper, we show how a systolic array of Multiply-and-Accumulate (MAC) units, similar to Google's Tensor Processing Unit (TPU), can be adapted to efficiently handle sparse matrices. TPU-like accelerators are built upon a 2D array of MAC units and have demonstrated high throughput and efficiency for dense matrix multiplication, which is a key kernel in machine learning algorithms and is the target of the TPU. In this work, we employ a co-designed approach of first developing a packing technique to condense a sparse matrix and then propose a systolic array based system, Sparse-TPU, abbreviated to STPU, to accommodate the matrix computations for the packed denser matrix counterparts. To demonstrate the efficacy of our co-designed approach, we evaluate sparse matrix-vector multiplication on a broad set of synthetic and real-world sparse matrices. Experimental results show that STPU delivers 16.08X higher performance while consuming 4.39X and 19.79X lower energy for integer (int8) and floating point (float32) implementations, respectively, over a TPU baseline. Meanwhile, STPU has 12.93% area overhead and an average of 4.14% increase in dynamic energy over the TPU baseline for the float32 implementation.},
booktitle = {Proceedings of the 34th ACM International Conference on Supercomputing},
articleno = {19},
numpages = {12},
keywords = {sparse matrix condensing, hardware-software codesign, hardware accelerators, application-specific hardware, systolic array, sparse matrix processing},
location = {Barcelona, Spain},
series = {ICS '20}
}

@misc{CPU-FPGA,
      title={Synergistic CPU-FPGA Acceleration of Sparse Linear Algebra}, 
      author={Mohammadreza Soltaniyeh and Richard P. Martin and Santosh Nagarakatte},
      year={2020},
      eprint={2004.13907},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{compBio,
      title={Distributed Many-to-Many Protein Sequence Alignment using Sparse Matrices}, 
      author={Oguz Selvitopi and Saliya Ekanayake and Giulia Guidi and Georgios Pavlopoulos and Ariful Azad and Aydin Buluc},
      year={2020},
      eprint={2009.14467},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@article{Kepner_2017,
   title={Enabling massive deep neural networks with the GraphBLAS},
   ISBN={9781538634721},
   url={http://dx.doi.org/10.1109/HPEC.2017.8091098},
   DOI={10.1109/hpec.2017.8091098},
   journal={2017 IEEE High Performance Extreme Computing Conference (HPEC)},
   publisher={IEEE},
   author={Kepner, Jeremy and Kumar, Manoj and Moreira, Jose and Pattnaik, Pratap and Serrano, Mauricio and Tufo, Henry},
   year={2017},
   month={Sep}
}

@book{jones,
author = {Jones, Neil D. and Gomard, Carsten K. and Sestoft, Peter},
title = {Partial Evaluation and Automatic Program Generation},
year = {1993},
isbn = {0130202495},
publisher = {Prentice-Hall, Inc.},
address = {USA}
}

@article{10.1145/1291220.1291199,
author = {Coutts, Duncan and Leshchinskiy, Roman and Stewart, Don},
title = {Stream Fusion: From Lists to Streams to Nothing at All},
year = {2007},
issue_date = {September 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/1291220.1291199},
doi = {10.1145/1291220.1291199},
abstract = {This paper presents an automatic deforestation system, stream fusion, based on equational transformations, that fuses a wider range of functions than existing short-cut fusion systems. In particular, stream fusion is able to fuse zips, left folds and functions over nested lists, including list comprehensions. A distinguishing feature of the framework is its simplicity: by transforming list functions to expose their structure, intermediate values are eliminated by general purpose compiler optimisations.We have reimplemented the Haskell standard List library on top of our framework, providing stream fusion for Haskell lists. By allowing a wider range of functions to fuse, we see an increase in the number of occurrences of fusion in typical Haskell programs. We present benchmarks documenting time and space improvements.},
journal = {SIGPLAN Not.},
month = oct,
pages = {315–326},
numpages = {12},
keywords = {program transformation, program fusion, program optimisation, deforestation, functional programming}
}

@inproceedings{fusion,
author = {Coutts, Duncan and Leshchinskiy, Roman and Stewart, Don},
title = {Stream Fusion: From Lists to Streams to Nothing at All},
year = {2007},
isbn = {9781595938152},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1291151.1291199},
doi = {10.1145/1291151.1291199},
abstract = {This paper presents an automatic deforestation system, stream fusion, based on equational transformations, that fuses a wider range of functions than existing short-cut fusion systems. In particular, stream fusion is able to fuse zips, left folds and functions over nested lists, including list comprehensions. A distinguishing feature of the framework is its simplicity: by transforming list functions to expose their structure, intermediate values are eliminated by general purpose compiler optimisations.We have reimplemented the Haskell standard List library on top of our framework, providing stream fusion for Haskell lists. By allowing a wider range of functions to fuse, we see an increase in the number of occurrences of fusion in typical Haskell programs. We present benchmarks documenting time and space improvements.},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming},
pages = {315–326},
numpages = {12},
keywords = {program transformation, program optimisation, deforestation, program fusion, functional programming},
location = {Freiburg, Germany},
series = {ICFP '07}
}


@article{graph2,
title = "The anatomy of a large-scale hypertextual Web search engine",
journal = "Computer Networks and ISDN Systems",
volume = "30",
number = "1",
pages = "107 - 117",
year = "1998",
note = "Proceedings of the Seventh International World Wide Web Conference",
issn = "0169-7552",
doi = "https://doi.org/10.1016/S0169-7552(98)00110-X",
url = "http://www.sciencedirect.com/science/article/pii/S016975529800110X",
author = "Sergey Brin and Lawrence Page",
keywords = "World Wide Web, Search engines, Information retrieval, PageRank, Google"
}
@INPROCEEDINGS{graph1,  author={M. {Besta} and F. {Marending} and E. {Solomonik} and T. {Hoefler}},  booktitle={2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},   title={SlimSell: A Vectorizable Graph Representation for Breadth-First Search},   year={2017},  volume={},  number={},  pages={32-41},  doi={10.1109/IPDPS.2017.93}}

@article{amazon,
author = {Linden, Greg and Smith, Brent and York, Jeremy},
title = {Amazon.Com Recommendations: Item-to-Item Collaborative Filtering},
year = {2003},
issue_date = {January 2003},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {7},
number = {1},
issn = {1089-7801},
url = {https://doi.org/10.1109/MIC.2003.1167344},
doi = {10.1109/MIC.2003.1167344},
abstract = {By comparing similar items rather than similar customers, item-to-item collaborative filtering scales to very large data sets and produces high-quality recommendations.},
journal = {IEEE Internet Computing},
month = jan,
pages = {76–80},
numpages = {5}
}

@misc{gupta2020architectural,
      title={The Architectural Implications of Facebook's DNN-based Personalized Recommendation}, 
      author={Udit Gupta and Carole-Jean Wu and Xiaodong Wang and Maxim Naumov and Brandon Reagen and David Brooks and Bradford Cottel and Kim Hazelwood and Bill Jia and Hsien-Hsin S. Lee and Andrey Malevich and Dheevatsa Mudigere and Mikhail Smelyanskiy and Liang Xiong and Xuan Zhang},
      year={2020},
      eprint={1906.03109},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@article{GraphIt,
author = {Zhang, Yunming and Yang, Mengjiao and Baghdadi, Riyadh and Kamil, Shoaib and Shun, Julian and Amarasinghe, Saman},
title = {GraphIt: A High-Performance Graph DSL},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {OOPSLA},
url = {https://doi.org/10.1145/3276491},
doi = {10.1145/3276491},
abstract = {The performance bottlenecks of graph applications depend not only on the algorithm and the underlying hardware, but also on the size and structure of the input graph. As a result, programmers must try different combinations of a large set of techniques, which make tradeoffs among locality, work-efficiency, and parallelism, to develop the best implementation for a specific algorithm and type of graph. Existing graph frameworks and domain specific languages (DSLs) lack flexibility, supporting only a limited set of optimizations. This paper introduces GraphIt, a new DSL for graph computations that generates fast implementations for algorithms with different performance characteristics running on graphs with different sizes and structures. GraphIt separates what is computed (algorithm) from how it is computed (schedule). Programmers specify the algorithm using an algorithm language, and performance optimizations are specified using a separate scheduling language. The algorithm language simplifies expressing the algorithms, while exposing opportunities for optimizations. We formulate graph optimizations, including edge traversal direction, data layout, parallelization, cache, NUMA, and kernel fusion optimizations, as tradeoffs among locality, parallelism, and work-efficiency. The scheduling language enables programmers to easily search through this complicated tradeoff space by composing together a large set of edge traversal, vertex data layout, and program structure optimizations. The separation of algorithm and schedule also enables us to build an autotuner on top of GraphIt to automatically find high-performance schedules. The compiler uses a new scheduling representation, the graph iteration space, to model, compose, and ensure the validity of the large number of optimizations. We evaluate GraphIt’s performance with seven algorithms on graphs with different structures and sizes. GraphIt outperforms the next fastest of six state-of-the-art shared-memory frameworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24 out of 32 experiments by up to 4.8\texttimes{}, and is never more than 43% slower than the fastest framework on the other experiments. GraphIt also reduces the lines of code by up to an order of magnitude compared to the next fastest framework.},
journal = {Proc. ACM Program. Lang.},
month = oct,
articleno = {121},
numpages = {30},
keywords = {Code Generation, Parallel Programming Languages, Compiler Optimizations, Domain Specific Languages, Graph Algorithms, Big Data}
}


@INPROCEEDINGS{OuterSpace,
  author={S. {Pal} and J. {Beaumont} and D. {Park} and A. {Amarnath} and S. {Feng} and C. {Chakrabarti} and H. {Kim} and D. {Blaauw} and T. {Mudge} and R. {Dreslinski}},
  booktitle={2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, 
  title={OuterSPACE: An Outer Product Based Sparse Matrix Multiplication Accelerator}, 
  year={2018},
  volume={},
  number={},
  pages={724-736},
  doi={10.1109/HPCA.2018.00067}}
  
@TECHREPORT{FPTTA,
    author = {Jon Mountjoy and Marcel Beemster},
    title = {Functional languages and very fine grained parallelism: Initial results},
    institution = {},
    year = {1994}
}


@article{StreamFusion,
author = {Kiselyov, Oleg and Biboudis, Aggelos and Palladinos, Nick and Smaragdakis, Yannis},
title = {Stream Fusion, to Completeness},
year = {2017},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/3093333.3009880},
doi = {10.1145/3093333.3009880},
journal = {SIGPLAN Not.},
month = jan,
pages = {285–299},
numpages = {15},
keywords = {multi-stage programming, optimization, Code generation, stream fusion, streams}
}

@inproceedings{StreamFusion2,
author = {Coutts, Duncan and Leshchinskiy, Roman and Stewart, Don},
year = {2007},
month = {09},
pages = {315-326},
title = {Stream Fusion. From Lists to Streams to Nothing at All},
volume = {42},
journal = {Sigplan Notices - SIGPLAN}
}

@article{WADLER1990231,
title = {Deforestation: transforming programs to eliminate trees},
journal = {Theoretical Computer Science},
volume = {73},
number = {2},
pages = {231-248},
year = {1990},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(90)90147-A},
url = {https://www.sciencedirect.com/science/article/pii/030439759090147A},
author = {Philip Wadler},
abstract = {An algorithm that transforms programs to eliminate intermediate trees is presented. The algorithm applies to any term containing only functions with definitions in a given syntactic form, and is suitable for incorporation in an optimizing compiler.}
}

@article{supercompilation,
author = {Sørensen, Morten and Glück, R. and Jones, Neil},
year = {1996},
month = {11},
pages = {811 - 838},
title = {A positive supercompiler},
volume = {6},
journal = {Journal of Functional Programming},
doi = {10.1017/S0956796800002008}
}

@inproceedings{distillation,
author = {Hamilton, Geoff},
year = {2009},
month = {06},
pages = {151-164},
title = {Extracting the Essence of Distillation},
isbn = {978-3-642-11485-4},
doi = {10.1007/978-3-642-11486-1_13}
}

@INPROCEEDINGS{qtree,  author={I. {Simecek}},  booktitle={2009 11th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing},   title={Sparse Matrix Computations Using the Quadtree Storage Format},   year={2009},  volume={},  number={},  pages={168-173},  doi={10.1109/SYNASC.2009.55}}

@misc{besta2019graph,
      title={Graph Processing on FPGAs: Taxonomy, Survey, Challenges}, 
      author={Maciej Besta and Dimitri Stanojevic and Johannes De Fine Licht and Tal Ben-Nun and Torsten Hoefler},
      year={2019},
      eprint={1903.06697},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}


@inproceedings{GraphProcessors,
  title={On the Design of an Efficient Hardware Accelerator for Large Scale Graph Analytics},
  author={Y. Horawalavithana},
  year={2016}
}

@INPROCEEDINGS{ACqua,  
author={R. {Coelho} and F. {Tanus} and A. {Moreira} and G. {Nazar}},  booktitle={2020 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)},   title={ACQuA: A Parallel Accelerator Architecture for Pure Functional Programs},   year={2020},  volume={},  number={},  pages={346-351},  doi={10.1109/ISVLSI49217.2020.00070}}


@InProceedings{PILGRIM,
author="Boeijink, Arjan
and H{\"o}lzenspies, Philip K. F.
and Kuper, Jan",
editor="Hage, Jurriaan
and Moraz{\'a}n, Marco T.",
title="Introducing the PilGRIM: A Processor for Executing Lazy Functional Languages",
booktitle="Implementation and Application of Functional Languages",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="54--71",
abstract="Processor designs specialized for functional languages received very little attention in the past 20 years. The potential for exploiting more parallelism and the developments in hardware technology, ask for renewed investigation of this topic. In this paper, we use ideas from modern processor architectures and the state of the art in compilation, to guide the design of our processor, the PilGRIM. We define a high-level instruction set for lazy functional languages and show the processor architecture, that can efficiently execute these instructions.",
isbn="978-3-642-24276-2"
}

@InProceedings{Reduceron,
author="Naylor, Matthew
and Runciman, Colin",
editor="Chitil, Olaf
and Horv{\'a}th, Zolt{\'a}n
and Zs{\'o}k, Vikt{\'o}ria",
title="The Reduceron: Widening the von Neumann Bottleneck for Graph Reduction Using an FPGA",
booktitle="Implementation and Application of Functional Languages",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="129--146",
abstract="For the memory intensive task of graph reduction, modern PCs are limited not by processor speed, but by the rate that data can travel between processor and memory. This limitation is known as the von Neumann bottleneck. We explore the effect of widening this bottleneck using a special-purpose graph reduction machine with wide, parallel memories. Our prototype machine -- the Reduceron -- is implemented using an FPGA, and is based on a simple template-instantiation evaluator. Running at only 91.5MHz on an FPGA, the Reduceron is faster than mature bytecode implementations of Haskell running on a 2.8GHz PC.",
isbn="978-3-540-85373-2"
}

@inproceedings{funcHLS,
  title={Compiling Irregular Software to Specialized Hardware},
  author={Richard Townsend},
  year={2019}
}

@article{hosc,
author = {Klyuchnikov, Ilya},
year = {2010},
month = {01},
pages = {},
title = {Supercompiler HOSC 1.5: homeomorphic embedding and generalization in a higher-order setting}
}

@inproceedings{smash,
author = {Kanellopoulos, Konstantinos and Vijaykumar, Nandita and Giannoula, Christina and Azizi, Roknoddin and Koppula, Skanda and Ghiasi, Nika Mansouri and Shahroodi, Taha and Luna, Juan Gomez and Mutlu, Onur},
title = {SMASH: Co-Designing Software Compression and Hardware-Accelerated Indexing for Efficient Sparse Matrix Operations},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358286},
doi = {10.1145/3352460.3358286},
abstract = {Important workloads, such as machine learning and graph analytics applications, heavily involve sparse linear algebra operations. These operations use sparse matrix compression as an effective means to avoid storing zeros and performing unnecessary computation on zero elements. However, compression techniques like Compressed Sparse Row (CSR) that are widely used today introduce significant instruction overhead and expensive pointer-chasing operations to discover the positions of the non-zero elements. In this paper, we identify the discovery of the positions (i.e., indexing) of non-zero elements as a key bottleneck in sparse matrix-based workloads, which greatly reduces the benefits of compression.We propose SMASH, a hardware-software cooperative mechanism that enables highly-efficient indexing and storage of sparse matrices. The key idea of SMASH is to explicitly enable the hardware to recognize and exploit sparsity in data. To this end, we devise a novel software encoding based on a hierarchy of bitmaps. This encoding can be used to efficiently compress any sparse matrix, regardless of the extent and structure of sparsity. At the same time, the bitmap encoding can be directly interpreted by the hardware. We design a lightweight hardware unit, the Bitmap Management Unit (BMU), that buffers and scans the bitmap hierarchy to perform highly-efficient indexing of sparse matrices. SMASH exposes an expressive and rich ISA to communicate with the BMU, which enables its use in accelerating any sparse matrix computation.We demonstrate the benefits of SMASH on four use cases that include sparse matrix kernels and graph analytics applications. Our evaluations show that SMASH provides average performance improvements of 38% for Sparse Matrix Vector Multiplication and 44% for Sparse Matrix Matrix Multiplication, over a state-of-the-art CSR implementation, on a wide variety of matrices with different characteristics. SMASH incurs a very modest hardware area overhead of up to 0.076% of an out-of-order CPU core.},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {600–614},
numpages = {15},
keywords = {compression, accelerators, specialized architectures, sparse matrices, memory, linear algebra, hardware-software cooperation, graph processing, efficiency},
location = {Columbus, OH, USA},
series = {MICRO '52}
}

@INPROCEEDINGS{LaGRAPH,  author={T. {Mattson} and T. A. {Davis} and M. {Kumar} and A. {Buluc} and S. {McMillan} and J. {Moreira} and C. {Yang}},  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},   title={LAGraph: A Community Effort to Collect Graph Algorithms Built on Top of the GraphBLAS},   year={2019},  volume={},  number={},  pages={276-284},  doi={10.1109/IPDPSW.2019.00053}}