\section{CFL-reachability in terms of linear algebra}
\label{sec:algo}

\subsection{Algorithm description}
The algorithm is based on the generalization of the FSM intersection for an RSM,  and the edge-labeled directed input graph.
Since the RSM is composed as a set of FSMs, it could easily be presented as an adjacency matrix for some graph over the set of labels.
As shown in the Definition~\ref{def:graph:adjproduct}, we can apply the Kronecker product for matrices to \textit{intersect} the RSM and the input graph to some extent.
But the RSM contains nonterminal symbols with the additional logic of \textit{recursive calls}, which requires a \textit{transitive closure} step to extract such symbols.

The core idea of the algorithm comes from the Kronecker product and transitive closure.
The algorithm boils down to the evaluation of the iterative Kronecker product for the adjacency matrix $\mathcal{M}_1$ of the RSM $R$ and the adjacency matrix $\mathcal{M}_2$ of the  input graph $\mathcal{G}$, followed by the transitive closure, extraction of new reachability information and updating the graph adjacency matrix $\mathcal{M}_2$. These steps are described in Algorithm~\ref{tensor:cflr}.

New elements of the Kronecker product are computed in Line 9 of the Algorithm~\ref{tensor:cflr}. Function $DTC(T, K)$ from Algorithm~\ref{tensor:cflr} takes transitive closure matrix $T$ and a matrix $K$ with edges to be inserted, maintains $T$ under edge insertions and returns pairs of vertices $(i,j)$ such that $j$ \textit {became reachable} from $i$ after the insertion of some edge from $K$. Then the new reachable pairs are validated in the lines 12-18: we are interested only in paths from start to final state of some box, therefore some pairs can be excluded from adding to $\mathcal{M}_2$. If $\mathcal{M}_2$ has changed after the insertion of the elements, we calculate the new elements of the Kronecker product and so on. 

Notice that Algorithm~\ref{tensor:cflr} naturally allows one to calculate regular reachability or FSM intersection (in this case the main while loop takes only one iteration to actually append data). This feature may be useful for regular over-approximation of the CFL-reachability, for example, when one needs to make the finding of the point-to information less expensive~\cite{10.1145/2814270.2814307, 10.1145/1103845.1094817}.

\begin{algorithm}[h]
\floatname{algorithm}{Listing}
\begin{algorithmic}[1]
\footnotesize
\caption{Kronecker product-based CFL-reachability}
\label{tensor:cflr}
\Function{LA-CFL-Reachability}{G, $\mathcal{G}$}
    % Input data preparation
    \State{$R \gets$ Recursive automata for $G$ with $r$ states}
    \State{$n \gets$ The number of vertices in $\mathcal{G}$}
    \State{$\mathcal{M}_1 \gets$ Adjacency matrix for $R$}
    \State{$\mathcal{M}_2 \gets$ Adjacency matrix for $\mathcal{G}$}
    \State{$\Delta\mathcal{M}_2 \gets \mathcal{M}_2 $}
     \State{$K, T \gets$ The empty matrices of size $rn \times rn$}
    \While{Matrix $\mathcal{M}_2$ is changing}
        % Kronecker product (i.e. partial intersection)
        \State{$K \gets \mathcal{M}_1 \otimes \Delta \mathcal{M}_2$}
        \Comment{Evaluate Kronecker product}
           \State{$\Delta \mathcal{M}_2 \gets$ The empty matrix}
        
       \State{$\Delta T \gets DTC(T, K)$}
      \Comment{Dynamic transitive closure, $\Delta T$ contains new reachable pairs}


      \For{$(i,j) \in  \Delta T $}
      \State{$s, f \gets \left\lfloor{i / r}\right\rfloor, \left\lfloor{j / r}\right\rfloor$}
      \State{$x, y \gets i \bmod n, j \bmod n$}
            \If{$s$ is start  state and $f$ is a final state for box $A$}  
          \Comment{Getting only accepting runs}
             \State{$\Delta \mathcal{M}_2[x, y] \gets \Delta \mathcal{M}_2[x, y] \cup \{A\}$}
            \EndIf
       \EndFor
        \State{$ \mathcal{M}_2 \gets \mathcal{M}_2 + \Delta \mathcal{M}_2$}
    \EndWhile
\State \Return $\mathcal{M}_2$
\EndFunction
\end{algorithmic}
\end{algorithm}


\paragraph*{Graph Kronecker product and machines intersection}

To effectively recompute the Kronecker product on each iteration, we employ the fact that it is left-distributive.
Let $\mathcal{A}_2$ be a matrix with newly added elements and $\mathcal{B}_2$ be a matrix with all previously found elements, such that $\mathcal{M}_2 = \mathcal{A}_2 + \mathcal{B}_2$.
Then by left-distributivity of the Kronecker product we have $K = \mathcal{M}_1 \otimes \mathcal{M}_2 = \mathcal{M}_1 \otimes (\mathcal{A}_2 + \mathcal{B}_2) = \mathcal{M}_1\otimes \mathcal{A}_2 + \mathcal{M}_1 \otimes \mathcal{B}_2$.
Note that $\mathcal{M}_1 \otimes \mathcal{B}_2$ is known from the previous iteration, so it is left to update some elements of $K$ by computing $\mathcal{M}_1\otimes \mathcal{A}_2$.

\paragraph*{Dynamic transitive closure}
Note that the adjacency matrix $\mathcal{M}_2$ is changed incrementally i.e. elements (edges) are added to $\mathcal{M}_2$ at each iteration of the algorithm and are never deleted from it.
So it is not necessary to recompute the whole product or transitive closure if some appropriate data structure is maintained.
The fast computation of transitive closure can be obtained by using an incremental transitive closure technique.
Let $T$ be a transitive closure matrix of the graph $\mathcal{G}$ with $n$ vertices.
We use an approach by Ibaraki and Katoh~\cite{IBARAKI198395} to maintain dynamic transitive closure.
The key idea of their algorithm is to recalculate reachability information only for those vertices which become reachable after insertion of a certain edge. For each newly inserted edge $(i, j)$ and every node $u \neq j$ of $G$ such that $T[u, i] = 1$ and $T[u, j]=0$, one needs to perform operation $T[u,v] = T[u, v] \wedge T[j, v]$ for every node $v$, where $1 \wedge 1 = 0 \wedge 0 = 1 \wedge 0 = 0$ and $0 \wedge 1 = 1$. In this way, transitive closure matrix $T$ can be maintained under edge insertions in $O(n^3)$ total time. 

We have modified this algorithm to achieve a logarithmic speed-up on a word RAM with word size $w= \theta(\log n)$. Notice that operations above are equivalent to the element-wise product of two vectors of size $n$, where multiplication operation is denoted as $\wedge$. To check whether $T[u, i] = 1$ and $T[u, j]=0$ one needs to multiply two vectors: the first vector represents reachability of the given vertex $i$ from other vertices $\{u_1, u_2, ..., u_n\}$ of the graph and the second vector represents the same for the given vertex $j$. The operation $T[u, v] \wedge T[j, v]$ also can be reduced to the computation of the element-wise product of two vectors of size $n$ for the given $u_k$. The first vector contains the information whether vertices  $\{v_1, v_2, ..., v_n\}$ of the graph are reachable from the given vertex $u_k$ and the second vector represents the same for the given vertex $j$. The element-wise product of two vectors can be calculated naively in time $O(n)$. Thus, the time complexity of the transitive closure can be reduced by speeding up the element-wise product of two vectors of size $n$.

To achieve logarithmic speed-up, we use the Four Russians' trick \cite{arlazarov1970economical}.
Let us assume an architecture with word size $w= \theta(\log n)$.
First, we split each vector into $n/\log n$ parts of size $\log n$.
Then we create a table $\mathcal{T}$ such that $\mathcal{T}(a, b)$ = $a \wedge b$ where $a, b \ \in {\{0,1\}}^{\log n}$.
This takes time $O(n^2 \log n)$, since there are $2^{\log n} = n$ variants of Boolean vectors of size $\log n$ and hence $n^2$ possible pairs of vectors $(a, b)$ in total, and each component takes $O(\log n)$ time.
Assuming constant-time logical operations on words, we can store a polynomial number of lookup tables (arrays) $\mathcal{T}_i$ (one array for each vector of size $\log n$), such that given an index of a table $\mathcal{T}_i$, and any $O(\log n)$ bit vector $b$, we can look up $\mathcal{T}_i(b)$ in constant time. The index of each array $\mathcal{T}_a$ is stored in array $\mathcal{T}$, which can be accessed in constant time for a given $\log$-size vector $a$. Thus, we can calculate the product of two parts $a$ and $b$ of size $\log n$ in constant time using the table $\mathcal{T}$.
There are $n/\log n$ such parts, so the element-wise product of two vectors of size $n$ can be calculated in time $O(n/\log n)$ with $O(n^2 \log n)$ preprocessing.



\subsection{Correctness and complexity}

\paragraph*{Correctness}
\begin{theorem}
    Let $\mathcal{G} = (V,E,L)$ be a graph and $G = \langle\Sigma, N, S, P\rangle$ be a grammar.
    Let $\mathcal{M}_{2}$ be a resulting adjacency matrix after the execution of the algorithm in Algorithm~\ref{tensor:cflr}. Then for any valid indices $i, j$ and for each nonterminal $A \in N$ the following statement holds: the non-terminal $A \in \mathcal{M}_2[i,j]$, iff there is a $A$-path from node $i$ to node $j$ in the graph $\mathcal{G}$.
\end{theorem}
\begin{proof}
    The main idea of the proof is to use induction on the height of the derivation tree obtained on each iteration.
\end{proof}


\paragraph*{Complexity}

\begin{theorem}{}
\label{theorem: subcubic}
    Let $\mathcal{G} = \langle V,E,L \rangle$ be a graph and $G = \langle\Sigma, N, S, P\rangle$ be a grammar.
    The Algorithm~\ref{tensor:cflr} calculates the resulting matrix $\mathcal{M}_2$ in $O({|P|}^3n^3/\log (|P|n))$ time on a word RAM with word size $w= \theta(\log |P|n)$, where $n = |V|$. Moreover, maintaining of the dynamic transitive closure dominates the cost of the algorithm.
\end{theorem}


\begin{proof}
The most time-consuming steps of the algorithm are the computations of the Kronecker product and transitive closure.

 Let $|\Delta\mathcal{M}_2|$ be the number of non-zero elements in a matrix $\Delta\mathcal{M}_2$. Consider the total time which is needed for computing the Kronecker products. The elements of the matrices $\Delta\mathcal{M}_2^{(i)}$ are pairwise distinct on every $i$-th iteration of the algorithm because $\Delta T$ contains only new reachable pairs of vertices.  Therefore the total number of operations is $\sum\limits_i{\text{\# of operations }(\mathcal{M}_1 \otimes \Delta\mathcal{M}_2^{(i)})} = |\mathcal{M}_1| \sum\limits_i {|\Delta\mathcal{M}_2^{(i)}|} = (|N| + |\Sigma|){|P|}^2 \sum\limits_i {|\Delta\mathcal{M}_2^{(i)}|} = O({(|N| + |\Sigma|)}^2{|P|}^2 n^2)$.

Now we derive the time complexity of maintaining the dynamic transitive closure.
Notice that $K$ has the size of the Kronecker product of $\mathcal{M}_1 \otimes \mathcal{M}_2$, which is equal to $r n \times r n = |P|n \times |P|n$ so no more than ${|P|}^2n^2$ edges will be added during all iterations of the Algorithm~\ref{tensor:cflr}.
Checking whether $T[u, i] = 1$ and $T[u, j]=0$ for every node $u \in V$ for each newly inserted edge $(i, j)$ requires one multiplication of vectors per insertion, thus total time is $O({|P|}^3n^3/\log (|P|n))$.
Note that after checking the condition, at least one element $T[u', j]$ changes value from 0 to 1 and then never becomes 0 for some $u'$ and $j$.
Therefore the operation $T[u',v] = T[u', v] \wedge T[j, v]$ for all $v \in V$ is executed at most once for every pair of vertices $(u',j)$ during the entire computation implying that the total time is equal to $O({|P|}^2n^2|P|n/\log (|P|n))=O({|P|}^3n^3/\log (|P|n))$, using the  multiplication of vectors.

The matrix $\Delta T$ contains only new elements, therefore $T$ can be updated directly using only $|\Delta T|$ operations and hence ${|P|}^2n^2$ operations in total.
The same holds for the lines 12-18 of the Algorithm~\ref{tensor:cflr}, because operations are performed only for non-zero elements of $|\Delta T|$.
Finally, the time complexity of the Algorithm~\ref{tensor:cflr} is $O({(|N| + |\Sigma|)}^2{|P|}^2 n^2) + O({|P|}^2n^2) + O({|P|}^2n^2 \log (|P|n)) + O({|P|}^3n^3/\log (|P|n)) + O({|P|}^2n^2)= O({|P|}^3n^3/\log (|P|n))$. 
\end{proof}
The complexity analysis of the Algorithm~\ref{tensor:cflr} shows that the maintaining of the incremental transitive closure dominates the cost of the algorithm. Thus, CFL-reachability can be solved in truly subcubic $O(n^{3-\varepsilon})$ time if there exists an incremental dynamic algorithm for the transitive closure for a graph with $n$ vertices with preprocessing time $O(n^{3-\varepsilon})$ and total update time $O(n^{3-\varepsilon})$. Unfortunately, such an algorithm is unlikely to exist: it was shown that there is no incremental dynamic transitive closure algorithm for a graph with $n$ vertices and at most $m$ edges with preprocessing time $poly(m)$, total update time $mn^{1-\varepsilon}$, and query time $m^{\delta-\varepsilon}$ for any $\delta \in (0, 1/2]$ per query that has an error probability of at most 1/3 assuming the widely believed Online Boolean Matrix-Vector Multiplication (OMv) Conjecture~\cite{10.1145/2746539.2746609}. OMv Conjecture states that for any constant $ \varepsilon>0$, there is no $O(n^{3-\varepsilon})$-time algorithm that solves OMv with an error probability of at most 1/3.
