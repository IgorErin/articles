%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
\documentclass[sigplan]{acmart}\settopmatter{printacmref=false, printfolios=false}


%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\copyrightyear{2020}
% \acmYear{2020}
% \setcopyright{rightsretained}
% \acmConference[PPoPP '20]{25th ACM SIGPLAN Symposium on Principles and
% Practice of Parallel Programming}{February 22--26, 2020}{San Diego, CA, USA}
% \acmBooktitle{25th ACM SIGPLAN Symposium on Principles and Practice of
% Parallel Programming (PPoPP '20), February 22--26, 2020, San Diego, CA, USA}
% \acmDOI{10.1145/3332466.3374507}
% \acmISBN{978-1-4503-6818-6/20/02}


%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%\citestyle{acmauthoryear}  %% For author/year citations
%\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

%%
\newcommand\question[1]{{\color{violet}#1}}
\newcommand\todo[1]{{\color{red}#1}}
%%

%%code things
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.97}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{
%   basicstyle=\ttfamily,
  mathescape
}

\usepackage{minted}

\begin{document}

%% Title information
\title[PROPOSAL: Sparse linear algebra hardware-software co-design]{PROPOSAL: Sparse linear algebra hardware-software co-design}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%\titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
%\subtitle{Subtitle}                     %% \subtitle is optional
%\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
% \author{Aleksey Tyurin}                                        %% can be repeated if necessary
% \orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
% \affiliation{
%   \position{Position1}
%   \department{Department1}              %% \department is recommended
%   \institution{Institution1}            %% \institution is required
%   \streetaddress{Street1 Address1}
%   \city{City1}
%   \state{State1}
%   \postcode{Post-Code1}
%   \country{Country1}                    %% \country is recommended
% }
% \email{first1.last1@inst1.edu}          %% \email is recommended

% %% Author with two affiliations and emails.
% \author{Daniil Berezun}
% %\authornote{with author2 note}          %% \authornote is optional;
%                                         %% can be repeated if necessary
% \orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
% \affiliation{
%   \position{Position2a}
%   \department{Department2a}             %% \department is recommended
%   \institution{Institution2a}           %% \institution is required
%   \streetaddress{Street2a Address2a}
%   \city{City2a}
%   \state{State2a}
%   \postcode{Post-Code2a}
%   \country{Country2a}                   %% \country is recommended
% }

% \email{first2.last2@inst2a.com}         %% \email is recommended

% \author{Semyon Grigorev}
% %\authornote{with author2 note}          %% \authornote is optional;
%                                         %% can be repeated if necessary
% \orcid{0000-0002-7966-0698}             %% \orcid is optional
% \affiliation{
%   \position{Associate Professor}
%   %\department{Department2a}             %% \department is recommended
%   \institution{Saint Petersburg State University}           %% \institution is required
%   \streetaddress{Universitetskaya nab. 7/9}
%   \city{St. Petersburg}
%   \postcode{199034}
%   \country{Russia}                   %% \country is recommended
% }
% \email{s.v.grigoriev@spbu.ru}         %% \email is recommended
% \affiliation{
%   \position{Researcher}
%   %\department{Department2b}             %% \department is recommended
%   \institution{JetBrains Research}           %% \institution is required
%   \streetaddress{Primorskiy prospekt 68-70, Building 1}
%   \city{St. Petersburg}
%   \postcode{197374}
%   \country{Russia}                   %% \country is recommended
% }
% \email{semyon.grigorev@jetbrains.com}         %% \email is recommended

\author{Aleksey Tyurin}
\affiliation{
  \institution{Saint Petersburg State University}
}
\email{alekseytyurinspb@gmail.com}

\author{Daniil Berezun}
\affiliation{%
  \institution{JetBrains Research}
}
\email{daniil.berezun@jetbrains.com}

\author{Semyon Grigorev}
\additionalaffiliation{%
  \institution{JetBrains Research}
}
\affiliation{
  \institution{Saint Petersburg State University}
}
\email{s.v.grigoriev@spbu.ru}

%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}

In the era of big data, computations are expected to be faster and less power-consuming in order to become more effective and affordable. Sparse linear algebra is a great framework for building graph-based algorithms in a uniform and optimization-amenable way. However, CPUs and GPUs currently running such algorithms are underutilized due to being too general-purposed for problems that include sparsity. Thus, an application-specific integrated circuit could speed sparse computations up, following the example of \textit{Google TPU's}. It is worth noting that such a circuit needs to be not self-contained to allow some expected optimizations to be made by a compiler or a framework itself. Finally, the optimizations should be easily definable in the language and as automated as possible, thus a careful simultaneous design of hardware and software is needed. The proposal describes the bottlenecks inherent to present sparse linear algebra framework implementations, summarizes the expected optimizations, and proposes a co-design approach to designing a highly-optimized sparse linear algebra framework. \textit{This is a work in progress and not yet present the final result}.

% While GPU utilization allows one to speed up computations to the orders of magnitude, memory management remains the bottleneck making it often a challenge to achieve the desired performance. Hence, different memory optimizations are leveraged to
% reduce the number of memory transactions and 
% make memory being used more effectively.
%Notably, memory optimizations are being the most significant problem: GPUs memory hierarchy implies certain limitations, thus making data memory allocation management nontrivial and memory to be utilized carefully.
%In the paper we propose an approach automating memory management utilizing partial evaluation, a program transformation technique that enables the data accesses to be precomputed, optimized, and embedded into the code, mitigating memory transactions.
% We propose an approach automating memory management utilizing partial evaluation, a program transformation technique that enables data accesses to be pre-computed, optimized, and embedded into the code, saving memory transactions.
%As an empirical evaluation of our approach we applied the technique to a straightforward CUDA C na\"ive string pattern matching algorithm implementation.
%Our experiments show that the transformed program is up to 8 times as efficient as the original one.
% An empirical evaluation of our approach shows that the transformed program could be up to 8 times as efficient as the original one in the case of CUDA C na\"ive string pattern matching algorithm implementation.   
\end{abstract}

%%Default
%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
%\begin{CCSXML}
%<ccs2012>
%<concept>
%<concept_id>10011007.10011006.10011008</concept_id>
%<concept_desc>Software and its engineering~General programming languages</concept_desc>
%<concept_significance>500</concept_significance>
%</concept>
%<concept>
%<concept_id>10003456.10003457.10003521.10003525</concept_id>
%<concept_desc>Social and professional topics~History of programming languages</concept_desc>
%<concept_significance>300</concept_significance>
%</concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Software and its engineering~General programming languages}
%\ccsdesc[300]{Social and professional topics~History of programming languages}

%%Not default

% \begin{CCSXML}
% <ccs2012>
%   <concept>
%       <concept_id>10011007.10011006.10011008</concept_id>
%       <concept_desc>Software and its engineering~General programming languages</concept_desc>
%       <concept_significance>500</concept_significance>
%       </concept>
%   <concept>
%       <concept_id>10011007.10011006.10011041.10011047</concept_id>
%       <concept_desc>Software and its engineering~Source code generation</concept_desc>
%       <concept_significance>500</concept_significance>
%       </concept>
%  </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Software and its engineering~General programming languages}
% \ccsdesc[500]{Software and its engineering~Source code generation}

%% End of generated code


%% Keywords
%% comma separated list
% \keywords{GPU, CUDA, Partial Evaluation}  %% \keywords are mandatory in final camera-ready submission

\copyrightyear{2020}
\acmYear{2020}
\setcopyright{rightsretained}
\acmConference[Proposal]{2020}{December}{2020}
% \acmBooktitle{25th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP '20), February 22--26, 2020, San Diego, CA, USA}
\acmDOI{}
\acmISBN{}

%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section*{Introduction}

Sparse linear algebra is a great tool for tackling graph-based problems and allows a definition of plenty of algorithms in terms of matrix and vector operations over some semiring for a huge area of applications including but not limited to graph analysis~\cite{GAILLA}, computational biology~\cite{compBio} and machine learning~\cite{Kepner_2017}.  \emph{GraphBLAS}~\cite{buluc2017graphblas} standard defines building blocks, that could be implemented in software or hardware or both, for graph algorithms to be built in the language of linear algebra. A motivating example for a practical need of those blocks could be seen in listing~\ref{listing:1}, which is taken from a \emph{Suite Sparse} talk\footnote{\url{https://www.oden.utexas.edu/about/events/1520}}. The example performs breadth-first graph traversal using matrix-vector multiplication over a semiring. An operation could be parameterized by a mask, specifying what exactly elements are of particular interest: in case of a masked matrix-vector multiplication only masked elements are nonzero. Masked operations are essentially the operations followed by element-wise multiplication, thus element-wise operations are also a part of the standard. Notably, the multiplication operation of the semiring from the example returns the index of the element in the vector and the plus operation could nondeterministically return any of its parameters. 
\begin{listing}
\caption{Breadth-first search example}
\label{listing:1}
\begin{center}

\begin{minted}{python}
#Sparse algebra BFS:
#times: if (A[i,j] != 0) A[i,j] × q(k) = k
#       else 0
#plus : any(x,y) = x or y randomly

q = [source]; 
parent = [0 for i in range(n)]; 
parent[source] = source;

while (q not empty)
    #masked matrix vector multiplication
    q<¬parent> = A*q #mask could be inverted
    #masked assignment
    parent<q> = q
\end{minted}
    
\end{center}
\end{listing}


GraphBlas standard has been gradually being implemented for CPU and GPUs platforms with each release showing a performance enhancement. Following the work of~\cite{yang2020graphblast}, a number of straightforward optimizations could be emphasized that allow the implementation to catch up with the expected performance.

\emph{Direction-based} optimization is in charge of either deciding to use sparse-vector or dense-vector operations for  matrix-vector multiplication. The current frontier during a graph traversal could grow while the mask of yet not-visited vertices becomes small, so it becomes more profitable to utilize masked dense operations instead of sparse-matrix sparse-vector operations.

\emph{Load-balancing} optimization chooses the most suitable distribution among the workers (e.g. threads) depending on the sparsity of the operands. Or schedules the operands in a way to save computations, e.g. first merge two small arrays instead of a small and a large array in order to not extend a large array overhead throughout all the computation.

\emph{Mask fusion}. Ahead-of-time masking could reduce the number of memory accesses in case of matrix-vector multiplication and prevent memory blow-up in case of matrix-matrix multiplication: the multiplication of two sparse matrices could produce an order of magnitude
more nonzeroes in the output matrix compared with the two input matrices, hence the mask could limit the output number of nonzeroes, thus preventing out-of-memory errors. In order to achieve such a behavior, a mask should be fused (i.e. transformed in a single operation) with the corresponding operation for the operation to perform computations only for the elements in the mask.  

\emph{Kernel fusion}. Mask fusion is a special case of kernel fusion. Kernel fusion is responsible for fusing arbitrary operations. In the case of functional programming fusion~\cite{fusion} could be thought of as eliminating allocation of intermediate values of function composition, most notably for lists, since they are the primary data structure of functional programming. In the listing~\ref{listing:3} append function \texttt{app} joins three lists, and fusion generates such code for function \texttt{f} that traverses each list only once. A simple motivating example for kernel fusion could be seen in listing~\ref{listing:2}. Masking, addition, and subtraction could be fused in one kernel code to prevent intermediate results allocation and redundant memory accesses for traversal for each or element-wise operation. The graph representation of operations could exploit operation properties, e.g., associative property, to perform the fusion. Further, the fusion for operations that could be represented as a composition of \texttt{map/reduce} operations are well-studied and could be effectively fused~\cite{KernelFusion,Futhark}.
\begin{listing}
\centering
\caption{Fusion of function composition}
\label{listing:3}
\begin{minted}{haskell}
app [] ls = []
app (x:xs) ls = x : app xs ls

-- call for this function

app xs (app zs ys)

-- is fused to the following function definition
-- that is specialized for three lists

f [] xs ys = g xs ys
f (x:xs) ys zs = x : f xs ys zs

g [] xs = xs
g x:xs ys = x : g xs ys

-- and a call
f xs zs ys

\end{minted}

\end{listing}
% \begin{minted}[escapeinside=||]{haskell}
% map f (map g list) |$\equiv$| map (f |$\circ$| g) list 
% \end{minted}


\begin{listing}
\centering
\caption{Kernel fusion example}
\label{listing:2}
\begin{minted}{python}
#Fusing together '+', '-', and masking
#prevents two intermediate matrices allocation

C<mask> = A + C - B
\end{minted}
\end{listing}

\emph{Specialization}. Generally, it is a program transformation optimization~\cite{jones} that exploits the knowledge of some of the operation parameters, hence it could optimize those parts of the operation that depend on the known pieces of information. In the case of \emph{NVIDIA CUDA} specialization assigns a specialized computation per a warp, since warps could be executed independently. This technique is well-described, e.g.,  in~\cite{CUDADMA}. An example of such optimization could be a matrix-vector multiplication in some cycle where the vector remains unchanged. Hence, the vector could be embedded in the operation itself avoiding vector-specific overheads, e.g, allocation or data transfer back-and-forth from host to device. Matrix-vector multiplication is essentially a linear combination of columns where each column has a corresponding coefficient from the vector. Thus, each warp could multiply it's assigned column by the \texttt{warp\_id's} element of the vector, which could be embedded in the code of the warp itself. Such embedding is generally a huge \texttt{warp\_id} \texttt{switch} statement, which incurs almost no overhead in the case of CUDA.

However, there are inevitable challenges both for the hardware running sparse algorithms and for the software, performing optimizations. 

For the former, both CPUs and GPUs remain underutilized when executing sparse operations due to high cache-miss rates, limited communication between processors~\cite{Song_2016}. Sparse algorithms are inherently memory-bound and thus results in poor GFLOPs number compared to peak GFLOPs theoretically available on modern devices, namely less than $0.2\%$ of theoretical performance is achieved as reported in~\cite{leskovec2016snap, Florida}.

For the latter, optimizations are hard to automate and perform in general. The runtime of graph kernels is dependent on the input data, so in a multiple iteration algorithm, it might be profitable to fuse two kernels in one iteration and two different kernels in a different iteration. Further, kernels are compelled to satisfy certain restriction to be fuseable: the absence of intermediate synchronization, same data access pattern, enough resources on the device to execute fused kernel. Once again, \texttt{map/reduce} kernels could be successfully fused, but it is unclear whether GraphBlas standard could be solely implemented in \texttt{map/reduce} terms. For specialization, it is better for the underlying hardware to be \emph{MIMD} for the workers to be completely independent, however GPU's architecture is \emph{SIMT} (or \emph{SPMT}), which prevents successful specialization in many cases.

Eventually, some application-specific integrated circuits have been designed to address the issues mentioned above, that basically provide hardware units for sparse matrix-matrix or matrix-vector multiplication~\cite{zhang2020sparch, Song_2016, Systolic,CPU-FPGA}. A brief overview could be found in~\cite{zhang2020sparch}. The implementation from~\cite{zhang2020sparch} greatly outperforms CPUs (Intel MKL\footnote{\url{https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html}}) and GPUs (cuSPARSE\footnote{\url{https://developer.nvidia.com/cusparse}}) solutions in terms of speed and power consumption. Despite high performance, such solutions do not yet provide a complete implementation of GraphBlas (or even a subset required for a concise BFS from the example) and seem to be too self-contained to split the operation into phases that could be optimized in the discussed sense. 

Thus, a solution that combines the hardware and software in co-design fashion considering both hardware and software bottlenecks could be proposed. The hardware should be general enough to allow software optimizations and the optimizations should be easily definable in the co-designed language. A possible approach to tackle the design problem is to use a co-design framework~\footnote{\url{http://openasip.org/}} and TTA processor architecture, which is highly parallel and customizable (allowing, e.g., MIMD computations). 

Currently, sparse algebra frameworks speed up graph data\-bases, e.g., RedisGraph~\footnote{https://oss.redislabs.com/redisgraph/}, computational biology and machine learning, and equipping cloud solutions that provide such services with the dedicated sparse hardware could both increase the performance of the queries and make the services more affordable by reducing power consumption following Google's TPUs~\footnote{\url{https://cloud.google.com/tpu}} example.   

%% Bibliography
\bibliography{bib}


%% Appendix
% \appendix
% \section{Appendix}

% Text of appendix \ldots

\end{document}