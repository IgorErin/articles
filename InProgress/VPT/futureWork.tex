\section{Future Work}

We show that distillation is a promising way to optimize linear algebra based programs, thus it is a promising a way to optimize machine learning and graph processing procedures.

In the future, first of all, we should close a technical debt and make the distiller more stable to handle all important cases: current implementation can not handle such important functions as matrix-matrix multiplication.
Along with it, we should improve the input language to make it more user-friendly.
The main challenge here is to find the balance between language expressivity and the practicality of distillation for it.
Having basic workflow implemented we should explore how to utilize distillation in the best way for each particular platform. 
For example, which level of distillation is the best for our particular problem and set of functions?
Can we exploit more parallelism using distillation?
Can we efficiently exploit the tail-modulo-cons property of the distilled program?
What are the limitations of distillation: whether all important cases can be handled?

When the language and the distiller will be stable enough, we plan to implement a full-featured generic linear algebra library power enough to express basic graph analysis algorithms and to create and train neural networks.
After that, a number of graph analysis algorithms and neural networks will be implemented and evaluated.

Along with it we plan to improve both FHW and Reduceron and compilers for it in order to make them mature enough to handle real-world examples.
For example, it is necessary to support out-of-chip memory.