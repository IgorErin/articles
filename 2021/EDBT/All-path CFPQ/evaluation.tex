\section{Evaluation}
%The goal of this evaluation is to investigate the applicability of the proposed algorithm to context-free path querying.
%We measured the execution time of the index creation which solves the reachability problem for both kinds of queries.
%The execution time for CFPQ was compared with Azimov's algorithm for CFPQ reachability.
%We also investigated the practical applicability of the paths extraction algorithm to both regular and context-free path queries.

For evaluation, we used a PC with Ubuntu 18.04 installed.
It has Intel core i7-6700 CPU, 3.4GHz, and DDR4 64Gb RAM.
We only measure the execution time of the algorithms themselves, thus we assume an input graph is loaded into RAM in the form of its adjacency matrix in the sparse format.
Note, that the time needed to load an input graph into the RAM is excluded from the time measurements.

\subsubsection{Data Preparation}

We use the graphs and respective queries $g_1$ and $geo$ from~\cite{10.1145/3398682.3399163} to evaluate the RedisGraph-based solution.
The graphs are loaded into the RedisGraph database so that each vertex has a unique property \verb|id| in the range $[0, \ldots, |V|-1]$, where $|V|$ is a number of vertices in the graph to load.
This allows us to generate queries for a start vertex set with specific size using templates.
The template for the $g_1$ query is provided in listing~\ref{lst:query_pattern_g1}.
Here \texttt{\{id\_from\}} and \texttt{\{id\_to\}} are placeholders for the lower and the upper bounds for \verb|id|.

\begin{algorithm}
\floatname{algorithm}{Listing}
\begin{algorithmic}[1]
\caption{Cypher query pattern for $g_1$}
\label{lst:query_pattern_g1}
\State PATH PATTERN S =  \par
 \hskip\algorithmicindent ()-/ [<:SubClassOf [$\sim$S | ()] :SubClassOf] \par
 \hskip\algorithmicindent | [<:Type [$\sim$S | ()] :Type] /->()
\State MATCH (src)-/ $\sim$S /->()
\State WHERE \{id\_from\} <= src.id and src.id <= \{id\_to\}
\State RETURN count(*)
\end{algorithmic}
\end{algorithm}

We implemented a query generator for the queries $g_1$ and $geo$ to create concrete queries for all the start sets which are used in the previous experiment.


\subsubsection{Evaluation Results}

We use $geo$ query for \textit{geospecies} graph as one of the hardest queries, and $g_1$ query for other graphs.
We measure time and memory consumption for each start set.

\begin{figure}[h]
\centering
\includegraphics[width=0.41\textwidth]{data/raw_redis/geospecies.pdf}
\caption{RedisGraph performance on \textit{geospecies} graph}
\label{fig:redis_geospecies_all}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.41\textwidth]{data/raw_redis/eclass_514en.pdf}
\caption{RedisGraph performance on \textit{eclass\_514en} graph}
\label{fig:redis_eclass_all}
\end{figure}

The execution time for all sets, except the set of size 10~000 for \textit{geospecies} graph (fig.~\ref{fig:redis_geospecies_all}), is less than 1 second.
Moreover, for smaller graph (\textit{eclass\_514en}), processing time is less than 0.2 second for all chunks (fig.~\ref{fig:redis_geospecies_all}).

Memory consumption for the big graphs \textit{eclass\_514en} and \textit{geospecies} is presented in figures~\ref{fig:redis_memory_eclass} and~\ref{fig:redis_memory_geospecies} respectively.
The amount of memory used depends on the graph and the query, but RedisGraph uses less that 50Mb of RAM to process graphs with relatively small chunks ($\leq 1000$).
Note that RedisGraph includes memory management system, thus we measured all allocated memory, not only the memory really used for the query evaluation.
As a result, we can conclude that the multiple-source CFPQ is significantly more memory efficient than creation of the complete reachability index and its filtering: processing the set of size 10~000 on \textit{geospecies} graph requires less than 200Mb, while full index creation requires 16Gb~\cite{10.1145/3398682.3399163}.

\begin{figure}[t]
\centering
\includegraphics[width=0.41\textwidth]{data/raw_memory/eclass_514en.pdf}
\caption{Memory consumption on \textit{eclass\_514en}}
\label{fig:redis_memory_eclass}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.41\textwidth]{data/raw_memory/geospecies.pdf}
\caption{Memory consumption on \textit{geospecies}}
\label{fig:redis_memory_geospecies}
\end{figure}

We also evaluate how chunking affects the performance on the all-pairs reachability problem.
We fix the size of a chunk to be 1000 for graphs of different sizes and measure time and memory required to process queries.
Namely, we evaluate the query which is similar to the query from the previous scenario, but it does not constraint vertices ids (it does not have the \texttt{WHERE} clause).
We measure total processing time (in seconds) and total required memory (in Mb).
Also, we compare our solution with the results of Arseniy Terekhov et al. from~\cite{10.1145/3398682.3399163} in which the Azimov's algorithm was naively integrated with RedisGraph without support of lazy query evaluation and query language.
Similar hardware and the same input graphs and queries were used.
Results are provided in table~\ref{tbl:redis_full_graph_processing}.

{\setlength{\tabcolsep}{0.25em}
\begin{table}
{
\caption{Full graph processing with chunks of size 1000}
\label{tbl:redis_full_graph_processing}
\small
\rowcolors{3}{black!2}{black!10}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Graph} & \multirow{2}{*}{\#V} & \multirow{2}{*}{\#E} & \multirow{2}{*}{Q} & \multicolumn{2}{c|}{Chunks}  &  {Mono~\cite{10.1145/3398682.3399163}}  \\
                       &                      &                      &                        & T (sec)  & Mem (Mb) &  T (sec)\\
\hline
\hline
core                   & 1323                 & 4342                 & $g_1$                  & 0.003  & 2                  &  0.004 \\
pathways               & 6238                 & 18 598               & $g_1$                  & 0.031  & 6                  &  0.011 \\
gohierarchy            & 45 007               & 980 218              & $g_1$                  & 0.847  & 62                  &  0.091 \\
enzyme                 & 48 815               & 109 695              & $g_1$                  & 0.698  & 13                  &  0.018 \\
eclass\_514en          & 239 111              & 523 727              & $g_1$                  & 18.825 & 35                   &  0.067 \\
geospecies             & 450 609              & 2 311 461            & $geo$                  & 80.979 & 196                  &  7.146 \\
go                     & 272 770              & 534 311              & $g_1$                  & 72.034 & 40                  &  0.604 \\
\hline
\end{tabular}
}
\end{table}
}

Although chunk-by-chunk processing is slower, it still requires reasonable time.
Moreover, if the chunk size is comparable with the graph size (\textit{core} and \textit{pathways} graphs), then the execution time is comparable with the monolithic processing.
Thus one can decrease execution time by increasing the chunk size.
On the other hand, even with relatively small chunks (\textit{eclass\_514}, \textit{go} and \textit{geospecies} graphs), when for chunk-by-chunk processing is 100 times slower, our results are still reasonable for some cases.
For example, it requires over 70 times less time for \textit{geospecies} graph processing than the solution of Jochem Kuijpers et al.~\cite{Kuijpers:2019:ESC:3335783.3335791} which is based on Neo4j and requires more than 6000 seconds.
Moreover, while the solution from~\cite{10.1145/3398682.3399163} requires huge amount of memory (more than 16Gb for \textit{geospecies} graph and $geo$ query), our solution requires only 196Mb.
We argue, that our solution is more suitable for general-purpose graph databases.
First of all, the core scenario when the set of start vertices is relatively small can be handled efficiently.
Second, all-pairs reachability, which is not a massive case, can be solved in reasonable time with low memory consumption.
One can easily tune our solution to get the optimal time and memory consumption for their specific case.
