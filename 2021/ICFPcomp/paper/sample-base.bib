@misc{TensorFlowXLA,
  title = {XLA: Optimizing Compiler for Machine Learning},
  howpublished = {\url{https://www.tensorflow.org/xla?hl=en}},
  note = {Accessed: 2020-12-28}
}


@ARTICLE{GoogleTPU,  author={S. {Cass}},  journal={IEEE Spectrum},   title={Taking AI to the edge: Google's TPU now comes in a maker-friendly package},   year={2019},  volume={56},  number={5},  pages={16-17},  doi={10.1109/MSPEC.2019.8701189}}

@inproceedings{barefoot,
  title={Netchain: Scale-free sub-rtt coordination},
  author={Jin, Xin and Li, Xiaozhou and Zhang, Haoyu and Foster, Nate and Lee, Jeongkeun and Soul{\'e}, Robert and Kim, Changhoon and Stoica, Ion},
  booktitle={15th $\{$USENIX$\}$ Symposium on Networked Systems Design and Implementation ($\{$NSDI$\}$ 18)},
  pages={35--49},
  year={2018}
}

@inproceedings{redgrave2018pixel,
  title={Pixel Visual Core: Google’s Fully Programmable Image Vision and AI Processor For Mobile Devices},
  author={Redgrave, Jason and Meixner, Albert and Goulding-Hotta, Nathan and Vasilyev, Artem and Shacham, Ofer},
  booktitle={Proc. IEEE Hot Chips Symp.(HCS)},
  pages={1--18},
  year={2018}
}
@article{halide,
author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr\'{e}do and Amarasinghe, Saman},
title = {Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2499370.2462176},
doi = {10.1145/2499370.2462176},
abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values.We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
journal = {SIGPLAN Not.},
month = jun,
pages = {519–530},
numpages = {12},
keywords = {gpu, image processing, locality, autotuning, optimization, parallelism, redundant computation, vectorization, compiler, domain specific language}
}

@inproceedings{10.1145/2491956.2462176,
author = {Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr\'{e}do and Amarasinghe, Saman},
title = {Halide: A Language and Compiler for Optimizing Parallelism, Locality, and Recomputation in Image Processing Pipelines},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491956.2462176},
doi = {10.1145/2491956.2462176},
abstract = {Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values.We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {519–530},
numpages = {12},
keywords = {autotuning, gpu, optimization, parallelism, vectorization, compiler, image processing, redundant computation, domain specific language, locality},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}



@INPROCEEDINGS{SuiteSparse,  author={T. A. {Davis}},  booktitle={2018 IEEE High Performance extreme Computing Conference (HPEC)},   title={Graph algorithms via SuiteSparse: GraphBLAS: triangle counting and K-truss},   year={2018},  volume={},  number={},  pages={1-6},  doi={10.1109/HPEC.2018.8547538}}

@INPROCEEDINGS{redis,  author={P. {Cailliau} and T. {Davis} and V. {Gadepally} and J. {Kepner} and R. {Lipman} and J. {Lovitz} and K. {Ouaknine}},  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},   title={RedisGraph --- GraphBLAS Enabled Graph Database},   year={2019},  volume={},  number={},  pages={285-286},  doi={10.1109/IPDPSW.2019.00054}}

@article{graph2,
title = "The anatomy of a large-scale hypertextual Web search engine",
journal = "Computer Networks and ISDN Systems",
volume = "30",
number = "1",
pages = "107 - 117",
year = "1998",
note = "Proceedings of the Seventh International World Wide Web Conference",
issn = "0169-7552",
doi = "https://doi.org/10.1016/S0169-7552(98)00110-X",
url = "http://www.sciencedirect.com/science/article/pii/S016975529800110X",
author = "Sergey Brin and Lawrence Page",
keywords = "World Wide Web, Search engines, Information retrieval, PageRank, Google"
}
@INPROCEEDINGS{graph1,  author={M. {Besta} and F. {Marending} and E. {Solomonik} and T. {Hoefler}},  booktitle={2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},   title={SlimSell: A Vectorizable Graph Representation for Breadth-First Search},   year={2017},  volume={},  number={},  pages={32-41},  doi={10.1109/IPDPS.2017.93}}

@article{amazon,
author = {Linden, Greg and Smith, Brent and York, Jeremy},
title = {Amazon.Com Recommendations: Item-to-Item Collaborative Filtering},
year = {2003},
issue_date = {January 2003},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {7},
number = {1},
issn = {1089-7801},
url = {https://doi.org/10.1109/MIC.2003.1167344},
doi = {10.1109/MIC.2003.1167344},
abstract = {By comparing similar items rather than similar customers, item-to-item collaborative filtering scales to very large data sets and produces high-quality recommendations.},
journal = {IEEE Internet Computing},
month = jan,
pages = {76–80},
numpages = {5}
}

@misc{gupta2020architectural,
      title={The Architectural Implications of Facebook's DNN-based Personalized Recommendation}, 
      author={Udit Gupta and Carole-Jean Wu and Xiaodong Wang and Maxim Naumov and Brandon Reagen and David Brooks and Bradford Cottel and Kim Hazelwood and Bill Jia and Hsien-Hsin S. Lee and Andrey Malevich and Dheevatsa Mudigere and Mikhail Smelyanskiy and Liang Xiong and Xuan Zhang},
      year={2020},
      eprint={1906.03109},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@book{GAILLA,
author = {Kepner, Jeremy and Gilbert, John},
title = {Graph Algorithms in the Language of Linear Algebra},
year = {2011},
isbn = {0898719909},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {Graphs are among the most important abstract data types in computer science, and the algorithms that operate on them are critical to modern life. Graphs have been shown to be powerful tools for modeling complex problems because of their simplicity and generality. Graph algorithms are one of the pillars of mathematics, informing research in such diverse areas as combinatorial optimization, complexity theory, and topology. Algorithms on graphs are applied in many ways in today s world - from Web rankings to metabolic networks, from finite element meshes to semantic graphs. The current exponential growth in graph data has forced a shift to parallel computing for executing graph algorithms. Implementing parallel graph algorithms and achieving good parallel performance have proven difficult. This book addresses these challenges by exploiting the well-known duality between a canonical representation of graphs as abstract collections of vertices and edges and a sparse adjacency matrix representation. This linear algebraic approach is widely accessible to scientists and engineers who may not be formally trained in computer science. The authors show how to leverage existing parallel matrix computation techniques and the large amount of software infrastructure that exists for these computations to implement efficient and scalable parallel graph algorithms. The benefits of this approach are reduced algorithmic complexity, ease of implementation, and improved performance. Graph Algorithms in the Language of Linear Algebra is the first book to cover graph algorithms accessible to engineers and scientists not trained in computer science but having a strong linear algebra background, enabling them to quickly understand and apply graph algorithms. It also covers array-based graph algorithms, showing readers how to express canonical graph algorithms using a highly elegant and efficient array notation and how to tap into the large range of tools and techniques that have been built for matrices and tensors; parallel array-based algorithms, demonstrating with examples how to easily implement parallel graph algorithms using array-based approaches, which enables readers to address much larger graph problems; and array-based theory for analyzing graphs, providing a template for using array-based constructs to develop new theoretical approaches for graph analysis. Audience: This book is suitable as the primary text for a class on linear algebraic graph algorithms and as either the primary or supplemental text for a class on graph algorithms for engineers and scientists without training in computer science. Contents: List of Figures; List of Tables; List of Algorithms; Preface; Acknowledgments; Part I: Algorithms: Chapter 1: Graphs and Matrices; Chapter 2: Linear Algebraic Notation and Definitions; Chapter 3: Connected Components and Minimum Paths; Chapter 4: Some Graph Algorithms in an Array-Based Language; Chapter 5: Fundamental Graph Algorithms; Chapter 6: Complex Graph Algorithms; Chapter 7: Multilinear Algebra for Analyzing Data with Multiple Linkages; Chapter 8: Subgraph Detection; Part II: Data: Chapter 9: Kronecker Graphs; Chapter 10: The Kronecker Theory of Power Law Graphs; Chapter 11: Visualizing Large Kronecker Graphs; Part III: Computation: Chapter 12: Large-Scale Network Analysis; Chapter 13: Implementing Sparse Matrices for Graph Algorithms; Chapter 14: New Ideas in Sparse Matrix-Matrix Multiplication; Chapter 15: Parallel Mapping of Sparse Computations; Chapter 16: Fundamental Questions in the Analysis of Large Graphs; Index.}
}

@article{buluc2017graphblas,
  title={The GraphBLAS C API Specification},
  author={Buluc, Aydin and Mattson, Timothy and McMillan, Scott and Moreira, Jose and Yang, Carl},
  journal={GraphBLAS. org, Tech. Rep.},
  year={2017}
}

@misc{yang2020graphblast,
      title={GraphBLAST: A High-Performance Linear Algebra-based Graph Framework on the GPU}, 
      author={Carl Yang and Aydin Buluc and John D. Owens},
      year={2020},
      eprint={1908.01407},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}
@article{KernelFusion,
   title={Optimizing CUDA code by kernel fusion: application on BLAS},
   volume={71},
   ISSN={1573-0484},
   url={http://dx.doi.org/10.1007/s11227-015-1483-z},
   DOI={10.1007/s11227-015-1483-z},
   number={10},
   journal={The Journal of Supercomputing},
   publisher={Springer Science and Business Media LLC},
   author={Filipovič, Jiří and Madzin, Matúš and Fousek, Jan and Matyska, Luděk},
   year={2015},
   month={Jul},
   pages={3934–3957}
}

@inproceedings{10.1145/3062341.3062354,
author = {Henriksen, Troels and Serup, Niels G. W. and Elsman, Martin and Henglein, Fritz and Oancea, Cosmin E.},
title = {Futhark: Purely Functional GPU-Programming with Nested Parallelism and in-Place Array Updates},
year = {2017},
isbn = {9781450349888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3062341.3062354},
doi = {10.1145/3062341.3062354},
abstract = { Futhark is a purely functional data-parallel array language that offers a machine-neutral programming model and an optimising compiler that generates OpenCL code for GPUs.  This paper presents the design and implementation of three key features of Futhark that seek a suitable middle ground with imperative approaches.  First, in order to express efficient code inside the parallel constructs, we introduce a simple type system for in-place updates that ensures referential transparency and supports equational reasoning.  Second, we furnish Futhark with parallel operators capable of expressing efficient strength-reduced code, along with their fusion rules.  Third, we present a flattening transformation aimed at enhancing the degree of parallelism that (i) builds on loop interchange and distribution but uses higher-order reasoning rather than array-dependence analysis, and (ii) still allows further locality-of-reference optimisations. Finally, an evaluation on 16 benchmarks demonstrates the impact of the language and compiler features and shows application-level performance competitive with hand-written GPU code. },
booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {556–571},
numpages = {16},
keywords = {compilers, functional language, parallel, GPGPU},
location = {Barcelona, Spain},
series = {PLDI 2017}
}

@article{Futhark,
author = {Henriksen, Troels and Serup, Niels G. W. and Elsman, Martin and Henglein, Fritz and Oancea, Cosmin E.},
title = {Futhark: Purely Functional GPU-Programming with Nested Parallelism and in-Place Array Updates},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/3140587.3062354},
doi = {10.1145/3140587.3062354},
abstract = { Futhark is a purely functional data-parallel array language that offers a machine-neutral programming model and an optimising compiler that generates OpenCL code for GPUs.  This paper presents the design and implementation of three key features of Futhark that seek a suitable middle ground with imperative approaches.  First, in order to express efficient code inside the parallel constructs, we introduce a simple type system for in-place updates that ensures referential transparency and supports equational reasoning.  Second, we furnish Futhark with parallel operators capable of expressing efficient strength-reduced code, along with their fusion rules.  Third, we present a flattening transformation aimed at enhancing the degree of parallelism that (i) builds on loop interchange and distribution but uses higher-order reasoning rather than array-dependence analysis, and (ii) still allows further locality-of-reference optimisations. Finally, an evaluation on 16 benchmarks demonstrates the impact of the language and compiler features and shows application-level performance competitive with hand-written GPU code. },
journal = {SIGPLAN Not.},
month = jun,
pages = {556–571},
numpages = {16},
keywords = {parallel, functional language, compilers, GPGPU}
}



@inproceedings{CUDADMA,
author = {Bauer, Michael and Cook, Henry and Khailany, Brucek},
title = {CudaDMA: Optimizing GPU Memory Bandwidth via Warp Specialization},
year = {2011},
isbn = {9781450307710},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2063384.2063400},
doi = {10.1145/2063384.2063400},
abstract = {As the computational power of GPUs continues to scale with Moore's Law, an increasing number of applications are becoming limited by memory bandwidth. We propose an approach for programming GPUs with tightly-coupled specialized DMA warps for performing memory transfers between on-chip and off-chip memories. Separate DMA warps improve memory bandwidth utilization by better exploiting available memory-level parallelism and by leveraging efficient inter-warp producer-consumer synchronization mechanisms. DMA warps also improve programmer productivity by decoupling the need for thread array shapes to match data layout. To illustrate the benefits of this approach, we present an extensible API, CudaDMA, that encapsulates synchronization and common sequential and strided data transfer patterns. Using CudaDMA, we demonstrate speedup of up to 1.37x on representative synthetic microbenchmarks, and 1.15x-3.2x on several kernels from scientific applications written in CUDA running on NVIDIA Fermi GPUs.},
booktitle = {Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {12},
numpages = {11},
location = {Seattle, Washington},
series = {SC '11}
}

@article{Song_2016,
   title={Novel graph processor architecture, prototype system, and results},
   ISBN={9781509035250},
   url={http://dx.doi.org/10.1109/HPEC.2016.7761635},
   DOI={10.1109/hpec.2016.7761635},
   journal={2016 IEEE High Performance Extreme Computing Conference (HPEC)},
   publisher={IEEE},
   author={Song, William S. and Gleyzer, Vitaliy and Lomakin, Alexei and Kepner, Jeremy},
   year={2016},
   month={Sep}
}

@misc{leskovec2016snap,
      title={SNAP: A General Purpose Network Analysis and Graph Mining Library}, 
      author={Jure Leskovec and Rok Sosic},
      year={2016},
      eprint={1606.07550},
      archivePrefix={arXiv},
      primaryClass={cs.SI}
}

@article{Florida,
author = {Davis, Timothy A. and Hu, Yifan},
title = {The University of Florida Sparse Matrix Collection},
year = {2011},
issue_date = {November 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {0098-3500},
url = {https://doi.org/10.1145/2049662.2049663},
doi = {10.1145/2049662.2049663},
abstract = {We describe the University of Florida Sparse Matrix Collection, a large and actively growing set of sparse matrices that arise in real applications. The Collection is widely used by the numerical linear algebra community for the development and performance evaluation of sparse matrix algorithms. It allows for robust and repeatable experiments: robust because performance results with artificially generated matrices can be misleading, and repeatable because matrices are curated and made publicly available in many formats. Its matrices cover a wide spectrum of domains, include those arising from problems with underlying 2D or 3D geometry (as structural engineering, computational fluid dynamics, model reduction, electromagnetics, semiconductor devices, thermodynamics, materials, acoustics, computer graphics/vision, robotics/kinematics, and other discretizations) and those that typically do not have such geometry (optimization, circuit simulation, economic and financial modeling, theoretical and quantum chemistry, chemical process simulation, mathematics and statistics, power networks, and other networks and graphs). We provide software for accessing and managing the Collection, from MATLAB™, Mathematica™, Fortran, and C, as well as an online search capability. Graph visualization of the matrices is provided, and a new multilevel coarsening scheme is proposed to facilitate this task.},
journal = {ACM Trans. Math. Softw.},
month = dec,
articleno = {1},
numpages = {25},
keywords = {multilevel algorithms, Graph drawing, sparse matrices, performance evaluation}
}

 @inproceedings{zhang2020sparch,
    title     = {SpArch: Efficient Architecture for Sparse Matrix Multiplication},
    author    = {Zhang, Zhekai and Wang, Hanrui and Han, Song and Dally, William J.},
    booktitle = {26th IEEE International Symposium on High Performance Computer Architecture (HPCA)},
    year      = {2020}
} 

@inproceedings{Systolic,
author = {He, Xin and Pal, Subhankar and Amarnath, Aporva and Feng, Siying and Park, Dong-Hyeon and Rovinski, Austin and Ye, Haojie and Chen, Yuhan and Dreslinski, Ronald and Mudge, Trevor},
title = {Sparse-TPU: Adapting Systolic Arrays for Sparse Matrices},
year = {2020},
isbn = {9781450379830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3392717.3392751},
doi = {10.1145/3392717.3392751},
abstract = {While systolic arrays are widely used for dense-matrix operations, they are seldom used for sparse-matrix operations. In this paper, we show how a systolic array of Multiply-and-Accumulate (MAC) units, similar to Google's Tensor Processing Unit (TPU), can be adapted to efficiently handle sparse matrices. TPU-like accelerators are built upon a 2D array of MAC units and have demonstrated high throughput and efficiency for dense matrix multiplication, which is a key kernel in machine learning algorithms and is the target of the TPU. In this work, we employ a co-designed approach of first developing a packing technique to condense a sparse matrix and then propose a systolic array based system, Sparse-TPU, abbreviated to STPU, to accommodate the matrix computations for the packed denser matrix counterparts. To demonstrate the efficacy of our co-designed approach, we evaluate sparse matrix-vector multiplication on a broad set of synthetic and real-world sparse matrices. Experimental results show that STPU delivers 16.08X higher performance while consuming 4.39X and 19.79X lower energy for integer (int8) and floating point (float32) implementations, respectively, over a TPU baseline. Meanwhile, STPU has 12.93% area overhead and an average of 4.14% increase in dynamic energy over the TPU baseline for the float32 implementation.},
booktitle = {Proceedings of the 34th ACM International Conference on Supercomputing},
articleno = {19},
numpages = {12},
keywords = {sparse matrix condensing, hardware-software codesign, hardware accelerators, application-specific hardware, systolic array, sparse matrix processing},
location = {Barcelona, Spain},
series = {ICS '20}
}

@misc{CPU-FPGA,
      title={Synergistic CPU-FPGA Acceleration of Sparse Linear Algebra}, 
      author={Mohammadreza Soltaniyeh and Richard P. Martin and Santosh Nagarakatte},
      year={2020},
      eprint={2004.13907},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{compBio,
      title={Distributed Many-to-Many Protein Sequence Alignment using Sparse Matrices}, 
      author={Oguz Selvitopi and Saliya Ekanayake and Giulia Guidi and Georgios Pavlopoulos and Ariful Azad and Aydin Buluc},
      year={2020},
      eprint={2009.14467},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@article{Kepner_2017,
   title={Enabling massive deep neural networks with the GraphBLAS},
   ISBN={9781538634721},
   url={http://dx.doi.org/10.1109/HPEC.2017.8091098},
   DOI={10.1109/hpec.2017.8091098},
   journal={2017 IEEE High Performance Extreme Computing Conference (HPEC)},
   publisher={IEEE},
   author={Kepner, Jeremy and Kumar, Manoj and Moreira, Jose and Pattnaik, Pratap and Serrano, Mauricio and Tufo, Henry},
   year={2017},
   month={Sep}
}

@book{jones,
author = {Jones, Neil D. and Gomard, Carsten K. and Sestoft, Peter},
title = {Partial Evaluation and Automatic Program Generation},
year = {1993},
isbn = {0130202495},
publisher = {Prentice-Hall, Inc.},
address = {USA}
}

@article{10.1145/1291220.1291199,
author = {Coutts, Duncan and Leshchinskiy, Roman and Stewart, Don},
title = {Stream Fusion: From Lists to Streams to Nothing at All},
year = {2007},
issue_date = {September 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/1291220.1291199},
doi = {10.1145/1291220.1291199},
abstract = {This paper presents an automatic deforestation system, stream fusion, based on equational transformations, that fuses a wider range of functions than existing short-cut fusion systems. In particular, stream fusion is able to fuse zips, left folds and functions over nested lists, including list comprehensions. A distinguishing feature of the framework is its simplicity: by transforming list functions to expose their structure, intermediate values are eliminated by general purpose compiler optimisations.We have reimplemented the Haskell standard List library on top of our framework, providing stream fusion for Haskell lists. By allowing a wider range of functions to fuse, we see an increase in the number of occurrences of fusion in typical Haskell programs. We present benchmarks documenting time and space improvements.},
journal = {SIGPLAN Not.},
month = oct,
pages = {315–326},
numpages = {12},
keywords = {program transformation, program fusion, program optimisation, deforestation, functional programming}
}

@inproceedings{fusion,
author = {Coutts, Duncan and Leshchinskiy, Roman and Stewart, Don},
title = {Stream Fusion: From Lists to Streams to Nothing at All},
year = {2007},
isbn = {9781595938152},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1291151.1291199},
doi = {10.1145/1291151.1291199},
abstract = {This paper presents an automatic deforestation system, stream fusion, based on equational transformations, that fuses a wider range of functions than existing short-cut fusion systems. In particular, stream fusion is able to fuse zips, left folds and functions over nested lists, including list comprehensions. A distinguishing feature of the framework is its simplicity: by transforming list functions to expose their structure, intermediate values are eliminated by general purpose compiler optimisations.We have reimplemented the Haskell standard List library on top of our framework, providing stream fusion for Haskell lists. By allowing a wider range of functions to fuse, we see an increase in the number of occurrences of fusion in typical Haskell programs. We present benchmarks documenting time and space improvements.},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Functional Programming},
pages = {315–326},
numpages = {12},
keywords = {program transformation, program optimisation, deforestation, program fusion, functional programming},
location = {Freiburg, Germany},
series = {ICFP '07}
}

@article{SMASH,
   title={SMASH},
   ISBN={9781450369381},
   url={http://dx.doi.org/10.1145/3352460.3358286},
   DOI={10.1145/3352460.3358286},
   journal={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
   publisher={ACM},
   author={Kanellopoulos, Konstantinos and Vijaykumar, Nandita and Giannoula, Christina and Azizi, Roknoddin and Koppula, Skanda and Ghiasi, Nika Mansouri and Shahroodi, Taha and Luna, Juan Gomez and Mutlu, Onur},
   year={2019},
   month={Oct}
}

@article{sakr2020future,
  title={The Future is Big Graphs! A Community View on Graph Processing Systems},
  author={Sakr, Sherif and Bonifati, Angela and Voigt, Hannes and Iosup, Alexandru and Ammar, Khaled and Angles, Renzo and Aref, Walid and Arenas, Marcelo and Besta, Maciej and Boncz, Peter A and others},
  journal={arXiv preprint arXiv:2012.06171},
  year={2020}
}

@Inbook{TCEToolset,
	author="J{\"a}{\"a}skel{\"a}inen, Pekka and Viitanen, Timo and Takala, Jarmo and Berg, Heikki",
	editor="Hussain, Waqar and Nurmi, Jari and Isoaho, Jouni and Garzia, Fabio",
	title="HW/SW Co-design Toolset for Customization of Exposed Datapath Processors",
	bookTitle="Computing Platforms for Software-Defined Radio",
	year="2017",
	publisher="Springer International Publishing",
	pages="147--164",
	isbn="978-3-319-49679-5",
	doi="10.1007/978-3-319-49679-5_8",
	url="https://doi.org/10.1007/978-3-319-49679-5_8"
}

@inproceedings{Edwards2019FHWP,
  title={FHW Project : High-Level Hardware Synthesis from Haskell Programs},
  author={S. Edwards and Martha A. Kim and Richard Townsend and Kuangya Zhai and L. Lairmore},
  year={2019}
}


@INPROCEEDINGS{SparseDNN,  author={Davis, Timothy A. and Aznaveh, Mohsen and Kolodziej, Scott},  booktitle={2019 IEEE High Performance Extreme Computing Conference (HPEC)},   title={Write Quick, Run Fast: Sparse Deep Neural Network in 20 Minutes of Development Time via SuiteSparse:GraphBLAS},   year={2019},  volume={},  number={},  pages={1-6},  doi={10.1109/HPEC.2019.8916550}}

@misc{SuiteSparseOnly,
  title = {Algorithm 9xx: SuiteSparse:GraphBLAS: graph algorithms in
the language of sparse linear algebra},
  howpublished = {\url{https://people.engr.tamu.edu/davis/publications_files/toms_graphblas.pdf}},
  author={Timothy A. Davis, Texas A\&M University, USA},
  note = {Accessed: 2021-06-06}
}


@article{WADLER1990231,
title = {Deforestation: transforming programs to eliminate trees},
journal = {Theoretical Computer Science},
volume = {73},
number = {2},
pages = {231-248},
year = {1990},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(90)90147-A},
url = {https://www.sciencedirect.com/science/article/pii/030439759090147A},
author = {Philip Wadler},
abstract = {An algorithm that transforms programs to eliminate intermediate trees is presented. The algorithm applies to any term containing only functions with definitions in a given syntactic form, and is suitable for incorporation in an optimizing compiler.}
}

@article{supercompilation,
author = {Sørensen, Morten and Glück, R. and Jones, Neil},
year = {1996},
month = {11},
pages = {811 - 838},
title = {A positive supercompiler},
volume = {6},
journal = {Journal of Functional Programming},
doi = {10.1017/S0956796800002008}
}

@inproceedings{distillation,
author = {Hamilton, Geoff},
year = {2009},
month = {06},
pages = {151-164},
title = {Extracting the Essence of Distillation},
isbn = {978-3-642-11485-4},
doi = {10.1007/978-3-642-11486-1_13}
}

@INPROCEEDINGS{qtree,  author={I. {Simecek}},  booktitle={2009 11th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing},   title={Sparse Matrix Computations Using the Quadtree Storage Format},   year={2009},  volume={},  number={},  pages={168-173},  doi={10.1109/SYNASC.2009.55}}


@article{hosc,
author = {Klyuchnikov, Ilya},
year = {2010},
month = {01},
pages = {},
title = {Supercompiler HOSC 1.5: homeomorphic embedding and generalization in a higher-order setting}
}

@inproceedings{funcHLS,
  title={Compiling Irregular Software to Specialized Hardware},
  author={Richard Townsend},
  year={2019}
}
@misc{YaccPOT,
  title = {Distillation benchmarks},
  howpublished = {\url{https://github.com/YaccConstructor/Distiller}},
  note = {Accessed: 2021-07-01}
}